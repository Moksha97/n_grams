{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEyn9kHdTRBRGFE48wfL5+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Moksha97/n_grams/blob/main/nlp_assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YCCXM6HF5MOa"
      },
      "outputs": [],
      "source": [
        "# import statements\n",
        "import re\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "def preprocessing(path):\n",
        "  splitted_text = []\n",
        "  updated_para = []\n",
        "  new_tokens = []\n",
        "  cleaned_tokens = []\n",
        "  prev_token = \"\"\n",
        "  res_tokens = []\n",
        "  with open(path, 'r') as file:\n",
        "    # split the text based on period and append the <s> and </s> tokens\n",
        "    # respectively.\n",
        "    for line in file:\n",
        "      line = line.strip()\n",
        "      line = \"<s> \"+ line + \" </s>\"\n",
        "      updated_para.append(line)\n",
        "\n",
        "    # based on new tokens created the updated paragraph\n",
        "    for para in updated_para:\n",
        "      splitted_text = re.split(r'[ .]', para)\n",
        "      splitted_text = [s for s in splitted_text if s]\n",
        "      for token in splitted_text:\n",
        "        new_tokens.append(token)\n",
        "\n",
        "    # if the start<s> and end </s> tokens are side by side with no tokens\n",
        "    # in between remove unnecessary tokens\n",
        "\n",
        "    for token in new_tokens:\n",
        "      if token == \"</s>\" and prev_token == \"<s>\":\n",
        "        cleaned_tokens.pop()\n",
        "      else:\n",
        "        if token == '<s>' or token == '</s>':\n",
        "          cleaned_tokens.append(token)\n",
        "        else:\n",
        "          if re.match(\"^[a-zA-Z]+$\", token):\n",
        "            cleaned_tokens.append(token)\n",
        "            prev_token = token\n",
        "    # add the tokens if it is of text or start or stop only\n",
        "    for token in cleaned_tokens:\n",
        "      if token == '<s>' or token == '</s>' or re.match(\"^[a-zA-Z]+$\", token):\n",
        "        res_tokens.append(token.lower())\n",
        "  return res_tokens"
      ],
      "metadata": {
        "id": "w9rMQqO_5TaU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get N- grams\n",
        "def get_N_Grams(tokens, n, unknown_tok = None):\n",
        "  length = len(tokens)\n",
        "  n_grams = []\n",
        "  if n==1:\n",
        "    for index in range(length-n+1):\n",
        "      temp = tuple(tokens[j] for j in range(index, index + n))\n",
        "      n_grams.append(temp)\n",
        "  if n == 2 and unknown_tok is not None:\n",
        "    # Handle bigrams with unknown token replacement\n",
        "    for index in range(length-n+1):\n",
        "      token1 = tokens[index]\n",
        "      token2 = tokens[index + 1]\n",
        "\n",
        "      # Replace unknown tokens with a specified value\n",
        "      if token1 in unknown_tok:\n",
        "          token1 = 'UNK'\n",
        "      if token2 in unknown_tok:\n",
        "          token2 = 'UNK'\n",
        "\n",
        "      bigram = (token1, token2)\n",
        "      n_grams.append(bigram)\n",
        "  return n_grams"
      ],
      "metadata": {
        "id": "S9aB0x9T5UW-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get unigram frequncies\n",
        "def getUnigramFreq(uni_gram_data, cap):\n",
        "    unigram_token_freq = {}\n",
        "    unigram_token_freq['UNK'] = 0\n",
        "    unknown_set = []\n",
        "    for tokens in uni_gram_data:\n",
        "      for token in tokens:\n",
        "        if token in unigram_token_freq:\n",
        "          unigram_token_freq[token] += 1\n",
        "        else:\n",
        "          unigram_token_freq[token] = 1\n",
        "\n",
        "    for token, freq in unigram_token_freq.items():\n",
        "      if freq <= cap:\n",
        "        unknown_set.append(token)\n",
        "        unigram_token_freq['UNK'] += freq\n",
        "    return unigram_token_freq, unknown_set"
      ],
      "metadata": {
        "id": "Y1h6cmK45Yjw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get bi gram frequencies\n",
        "def getBigramFreq(bi_gram_data):\n",
        "  bigram_token_freq = {}\n",
        "  for token in bi_gram_data:\n",
        "    if token in bigram_token_freq:\n",
        "      bigram_token_freq[token] += 1\n",
        "    else:\n",
        "      bigram_token_freq[token] = 1\n",
        "  return bigram_token_freq"
      ],
      "metadata": {
        "id": "JZslZGdB5u3Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate 'N' except start token\n",
        "def getLength(uni_gram_freq = {}):\n",
        "  len = 0\n",
        "  for token,count in uni_gram_freq.items():\n",
        "    if token == '<s>':\n",
        "      continue\n",
        "    len += count\n",
        "  return len"
      ],
      "metadata": {
        "id": "Y0JagzFo5cLH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get probabilities with or without smoothing\n",
        "def getProbabilities(n, k, vocabSize, isSmoothing, uni_gram_freq = {}, bi_gram_freq = {}):\n",
        "    token_prob = {}\n",
        "    if not isSmoothing:\n",
        "      if n==1:\n",
        "        for token,count in uni_gram_freq.items():\n",
        "          token_prob[token] = count/vocabSize\n",
        "        return token_prob\n",
        "      elif n==2:\n",
        "        for token,count in bi_gram_freq.items():\n",
        "          token_prob[token] = count/uni_gram_freq[token[0]]\n",
        "        return token_prob\n",
        "    else:\n",
        "      if n == 1:\n",
        "        for token,count in uni_gram_freq.items():\n",
        "          token_prob[token] = ((count + k)/ (getLength(uni_gram_freq) + (k*vocabSize)))\n",
        "        return token_prob\n",
        "      if n == 2:\n",
        "        for token,count in bi_gram_freq.items():\n",
        "          token_prob[token] = ((count + k)/ (uni_gram_freq[token[0]] + (k*vocabSize)))\n",
        "        return token_prob"
      ],
      "metadata": {
        "id": "n_WhcbNL5hZJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get unigram probability of test data\n",
        "def getTestUnigramProb(test_data, train_prob):\n",
        "  test_uni_prob = {}\n",
        "  for tokens in test_data:\n",
        "    for token in tokens:\n",
        "      if token in train_prob:\n",
        "        test_uni_prob[token] = train_prob[token]\n",
        "      else:\n",
        "        test_uni_prob[token] = train_prob['UNK']\n",
        "  return test_uni_prob"
      ],
      "metadata": {
        "id": "cia4azeS5iU9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get bigram probability of test data\n",
        "def getTestBiGramProb(test_data, train_prob, unknown_tok):\n",
        "  test_bi_prob = {}\n",
        "  for tokens in test_data:\n",
        "    token1, token2 = tokens\n",
        "    token_1 = token1\n",
        "    token_2 = token2\n",
        "    if token1 in unknown_tok:\n",
        "      token_1 = 'UNK'\n",
        "    if token2 in unknown_tok:\n",
        "      token_2 = 'UNK'\n",
        "    # Check if the bigram exists in train_bigram_prob, otherwise use 'UNK' probability\n",
        "    if (token_1, token_2) in train_prob:\n",
        "        test_bi_prob[(token1, token2)] = train_prob[(token_1, token_2)]\n",
        "    else:\n",
        "        test_bi_prob[(token1, token2)] = train_prob.get(('UNK', 'UNK'), 0.0)\n",
        "  return test_bi_prob"
      ],
      "metadata": {
        "id": "tIAyzcZK6tWd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calcalutes perplexity\n",
        "def calcPerplexity(prob, n):\n",
        "  logsum = 0\n",
        "  for val in prob.values():\n",
        "    logsum += np.log(val)\n",
        "  return np.exp(-logsum/n)"
      ],
      "metadata": {
        "id": "2S0mr88b7UQV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file_path = \"./train.txt\"\n",
        "training_data = preprocessing(train_file_path)\n",
        "val_file_path = \"./val.txt\"\n",
        "val_data = preprocessing(val_file_path)"
      ],
      "metadata": {
        "id": "CROQc0yM7ks4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_uni_gram = get_N_Grams(training_data, 1)\n",
        "unigram_token_freq, unknown_tokens = getUnigramFreq(train_uni_gram,1)"
      ],
      "metadata": {
        "id": "wrEdye8P_HTD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_bi_gram = get_N_Grams(training_data, 2, unknown_tokens)\n",
        "bigram_token_freq = getBigramFreq(train_bi_gram)"
      ],
      "metadata": {
        "id": "5lLOTdBr_fG1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_unigram_prob = getProbabilities(1,0,len(train_uni_gram), False, unigram_token_freq, {})\n",
        "train_bigram_prob = getProbabilities(2,0,len(train_uni_gram), False, unigram_token_freq, bigram_token_freq)"
      ],
      "metadata": {
        "id": "EbRTQqO1Dcvy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_uni_grams = get_N_Grams(val_data, 1)\n",
        "test_bi_grams = get_N_Grams(val_data, 2, unknown_tokens)"
      ],
      "metadata": {
        "id": "33KApaTrCTdE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_uni_prob = getTestUnigramProb(test_uni_grams, train_unigram_prob)"
      ],
      "metadata": {
        "id": "S6xk2xCcDICd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_bi_prob = getTestBiGramProb(test_bi_grams,train_bigram_prob, unknown_tokens)"
      ],
      "metadata": {
        "id": "TCJ-dhowD7Cn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_unigram_perplexity = calcPerplexity(test_uni_prob, len(test_uni_prob))\n",
        "print(\"Validation Set Perplexity:\")\n",
        "print(f'The perplexity of unigram with unknown word handling is {test_unigram_perplexity}')\n",
        "test_bigram_perplexity = calcPerplexity(test_bi_prob, len(test_bi_prob))\n",
        "print(f'The perplexity of bigram with unknown word handling is {test_bigram_perplexity}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBrPKqtPFFAX",
        "outputId": "04bf23a5-5b5c-4e39-c38a-2cd26625f3f9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Set Perplexity:\n",
            "The perplexity of unigram with unknown word handling is 3412.237774835694\n",
            "The perplexity of bigram with unknown word handling is 30.273961210567073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# laplace smoothing - 1\n",
        "train_bigram_laplace_prob = getProbabilities(2,1,len(train_uni_gram), True, unigram_token_freq, bigram_token_freq)\n",
        "train_unigram_laplace_prob = getProbabilities(1,1,len(train_uni_gram), True, unigram_token_freq, bigram_token_freq)"
      ],
      "metadata": {
        "id": "Va9NDUsoFOqv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_uni_prob_Laplace = getTestUnigramProb(test_uni_grams, train_unigram_laplace_prob)"
      ],
      "metadata": {
        "id": "wc0nklbeHAU7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_bi_prob_Laplace = getTestBiGramProb(test_bi_grams, train_bigram_laplace_prob, unknown_tokens)"
      ],
      "metadata": {
        "id": "x6rDgH0ZIFDw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_unigram_perplexity_laplace = calcPerplexity(test_uni_prob_Laplace, len(test_uni_prob_Laplace))\n",
        "test_bigram_perplexity_laplace = calcPerplexity(test_bi_prob_Laplace, len(test_bi_prob_Laplace))\n",
        "print(\"Validation Set Perplexity:\")\n",
        "print(f'The perplexity of unigram with laplace smoothing is {test_unigram_perplexity_laplace}')\n",
        "print(f'The perplexity of bigram with laplace smoothing is {test_bigram_perplexity_laplace}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Epr2nfsIfcN",
        "outputId": "decf7ce8-3261-490c-bb8e-e5505d80ae4c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Set Perplexity:\n",
            "The perplexity of unigram with laplace smoothing is 5875.292572274528\n",
            "The perplexity of bigram with laplace smoothing is 2922.721304257471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# K-smoothing\n",
        "k_Smooth = [0.01, 0.05, 0.1, 0.5, 0.75]\n",
        "k_Smoothing_bigram = []\n",
        "for k in range(len(k_Smooth)):\n",
        "  k_Smoothing_bigram.append(getProbabilities(2,k_Smooth[k],len(train_uni_gram), True, unigram_token_freq, bigram_token_freq))\n",
        "\n",
        "test_bigram_prob_ksmooth = []\n",
        "for ksm in range(len(k_Smoothing_bigram)):\n",
        "  test_bigram_prob_ksmooth.append(getTestBiGramProb(test_bi_grams, k_Smoothing_bigram[ksm], unknown_tokens))\n",
        "print(\"Validation Set Perplexity:\")\n",
        "for ksm in range(len(test_bigram_prob_ksmooth)):\n",
        "  test_k_smooth_perplexity = None\n",
        "  test_k_smooth_perplexity = calcPerplexity(test_bigram_prob_ksmooth[ksm],len(test_bigram_prob_ksmooth[ksm]))\n",
        "  print(f'The perplexity of bigram with k-smoothing of k = {k_Smooth[ksm]} is {test_k_smooth_perplexity}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv7HWuhjJCNc",
        "outputId": "502d4487-2f04-4e1c-8797-75692cd8c2f5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Set Perplexity:\n",
            "The perplexity of bigram with k-smoothing of k = 0.01 is 90.05990983589889\n",
            "The perplexity of bigram with k-smoothing of k = 0.05 is 234.85621316673965\n",
            "The perplexity of bigram with k-smoothing of k = 0.1 is 401.47874343840914\n",
            "The perplexity of bigram with k-smoothing of k = 0.5 is 1609.247564015122\n",
            "The perplexity of bigram with k-smoothing of k = 0.75 is 2288.062284588195\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Set Perplexity:\")\n",
        "train_unigram_perplexity = calcPerplexity(train_unigram_prob, len(train_unigram_prob))\n",
        "print(f'The perplexity of unigram with unknown word handling is {train_unigram_perplexity}')\n",
        "train_bigram_perplexity = calcPerplexity(train_bigram_prob, len(train_bigram_prob))\n",
        "print(f'The perplexity of bigram with unknown word handling is {train_bigram_perplexity}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBtR9ix6RAB9",
        "outputId": "1230fc98-9a7f-406f-b0e4-22fb05613a0d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Perplexity:\n",
            "The perplexity of unigram with unknown word handling is 30040.634286141903\n",
            "The perplexity of bigram with unknown word handling is 50.42475001455599\n"
          ]
        }
      ]
    }
  ]
}